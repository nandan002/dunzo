{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DUNZo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvuYOz0Ykkov"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBkuyDSCku4F"
      },
      "source": [
        "!mkdir images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbboQrgElHUN"
      },
      "source": [
        "# List of no logo dataset\n",
        "import os\n",
        "files=os.listdir('/content/drive/MyDrive/DUNZO/nologo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XT-W_Q6lJ-j",
        "outputId": "adc93b84-ce3c-4497-a9c9-da7ec1224c5b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u23O-A6lZxO"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator,img_to_array,array_to_img,load_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl_hXHV4ln0r"
      },
      "source": [
        "# Image Data generator to create multiple images from less data\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLZm99Y3mLSL"
      },
      "source": [
        "# Creating data augmentation and storing the images to increase the dataset size\n",
        "# Storing the images folder wise so that it can be passed through ImageDataGenerator\n",
        "for f in files:\n",
        "  img = load_img('/content/drive/MyDrive/DUNZO/nologo/'+f)  \n",
        "  x = img_to_array(img) \n",
        "  x = x.reshape((1,) + x.shape) \n",
        "\n",
        "  i = 0\n",
        "  for batch in datagen.flow(x, batch_size=1,\n",
        "                          save_to_dir='/content/drive/MyDrive/DUNZO/nologo', save_prefix='dunzo', save_format='jpeg'):\n",
        "    i += 1\n",
        "    if i > 5:\n",
        "        break   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPTZV672fzSV"
      },
      "source": [
        "!unzip /content/drive/MyDrive/DUNZO.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58L-jAOtOY2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoG0jUEpcmIb"
      },
      "source": [
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKWzx4l69VP2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpXnU2Ivc9zi",
        "outputId": "5b1f9242-4fc8-4e97-9ea8-36f896b1b291"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# this is the augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation for testing:\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/DUNZO/train',  \n",
        "        target_size=(150, 150),  \n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')  \n",
        "\n",
        "# for validation data\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/content/DUNZO/validation',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1397 images belonging to 2 classes.\n",
            "Found 341 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJn5X6kzhc6r"
      },
      "source": [
        "# Callbacks to save the best model and early stop to stop if accuracy does not increase\n",
        "checkpoint_filepath = '/content/checkpoint'\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "earlystop=keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=5,restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uawB9fp4dV8k",
        "outputId": "9be2fd0c-a35c-4e84-fe11-8bc0392d4718"
      },
      "source": [
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=1397 // batch_size,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=341 // batch_size,\n",
        "        callbacks=[model_checkpoint_callback,earlystop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "60/87 [===================>..........] - ETA: 4s - loss: 0.4598 - accuracy: 0.8335"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87/87 [==============================] - 16s 177ms/step - loss: 0.4234 - accuracy: 0.8631 - val_loss: 0.3521 - val_accuracy: 0.9018\n",
            "Epoch 2/50\n",
            "87/87 [==============================] - 15s 173ms/step - loss: 0.1707 - accuracy: 0.9471 - val_loss: 0.1600 - val_accuracy: 0.9494\n",
            "Epoch 3/50\n",
            "87/87 [==============================] - 15s 172ms/step - loss: 0.1454 - accuracy: 0.9602 - val_loss: 0.1566 - val_accuracy: 0.9554\n",
            "Epoch 4/50\n",
            "87/87 [==============================] - 15s 176ms/step - loss: 0.1344 - accuracy: 0.9667 - val_loss: 0.5009 - val_accuracy: 0.9107\n",
            "Epoch 5/50\n",
            "87/87 [==============================] - 15s 173ms/step - loss: 0.1670 - accuracy: 0.9645 - val_loss: 0.0683 - val_accuracy: 0.9702\n",
            "Epoch 6/50\n",
            "87/87 [==============================] - 15s 173ms/step - loss: 0.1444 - accuracy: 0.9696 - val_loss: 0.1303 - val_accuracy: 0.9643\n",
            "Epoch 7/50\n",
            "87/87 [==============================] - 15s 171ms/step - loss: 0.0915 - accuracy: 0.9841 - val_loss: 0.0153 - val_accuracy: 0.9940\n",
            "Epoch 8/50\n",
            "87/87 [==============================] - 15s 172ms/step - loss: 0.1055 - accuracy: 0.9855 - val_loss: 0.1062 - val_accuracy: 0.9851\n",
            "Epoch 9/50\n",
            "87/87 [==============================] - 15s 172ms/step - loss: 0.1671 - accuracy: 0.9703 - val_loss: 0.2776 - val_accuracy: 0.9405\n",
            "Epoch 10/50\n",
            "87/87 [==============================] - 15s 171ms/step - loss: 0.1524 - accuracy: 0.9696 - val_loss: 0.1963 - val_accuracy: 0.9524\n",
            "Epoch 11/50\n",
            "87/87 [==============================] - 15s 171ms/step - loss: 0.1269 - accuracy: 0.9761 - val_loss: 0.1422 - val_accuracy: 0.9702\n",
            "Epoch 12/50\n",
            "87/87 [==============================] - 15s 171ms/step - loss: 0.1041 - accuracy: 0.9826 - val_loss: 0.0231 - val_accuracy: 0.9970\n",
            "Epoch 13/50\n",
            "87/87 [==============================] - 15s 171ms/step - loss: 0.0949 - accuracy: 0.9826 - val_loss: 0.0284 - val_accuracy: 0.9911\n",
            "Epoch 14/50\n",
            "87/87 [==============================] - 15s 172ms/step - loss: 0.0983 - accuracy: 0.9870 - val_loss: 0.0320 - val_accuracy: 0.9881\n",
            "Epoch 15/50\n",
            "87/87 [==============================] - 15s 170ms/step - loss: 0.0681 - accuracy: 0.9819 - val_loss: 0.2536 - val_accuracy: 0.9762\n",
            "Epoch 16/50\n",
            "87/87 [==============================] - 15s 172ms/step - loss: 0.0951 - accuracy: 0.9877 - val_loss: 0.3896 - val_accuracy: 0.9435\n",
            "Epoch 17/50\n",
            "87/87 [==============================] - 15s 170ms/step - loss: 0.1238 - accuracy: 0.9855 - val_loss: 0.4357 - val_accuracy: 0.9494\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f015eaa75d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cDhCsKij4x9"
      },
      "source": [
        "# Saving the model\n",
        "model.save('best_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}